{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3625893,"sourceType":"datasetVersion","datasetId":2167226}],"dockerImageVersionId":30579,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional","metadata":{"id":"b657MzBjzA77","execution":{"iopub.status.busy":"2023-11-15T12:42:38.516430Z","iopub.execute_input":"2023-11-15T12:42:38.516847Z","iopub.status.idle":"2023-11-15T12:42:38.523743Z","shell.execute_reply.started":"2023-11-15T12:42:38.516816Z","shell.execute_reply":"2023-11-15T12:42:38.522576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = \"/kaggle/input/ted-talk-youtube/TED.csv\"\ndf = pd.read_csv(file_path)\ndf.head()","metadata":{"id":"4_qsx85-5Gbh","outputId":"c2eed248-0dec-4fd9-ac26-fb54493e20ca","execution":{"iopub.status.busy":"2023-11-15T12:42:38.525851Z","iopub.execute_input":"2023-11-15T12:42:38.526986Z","iopub.status.idle":"2023-11-15T12:42:38.650566Z","shell.execute_reply.started":"2023-11-15T12:42:38.526940Z","shell.execute_reply":"2023-11-15T12:42:38.649105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Raw_list = df['title'].tolist()\nRaw_list[:3]","metadata":{"id":"5EFWWlXlmIhN","outputId":"7281f93e-a73e-4341-976d-78caf52fd93b","execution":{"iopub.status.busy":"2023-11-15T12:42:38.652113Z","iopub.execute_input":"2023-11-15T12:42:38.652461Z","iopub.status.idle":"2023-11-15T12:42:38.662501Z","shell.execute_reply.started":"2023-11-15T12:42:38.652432Z","shell.execute_reply":"2023-11-15T12:42:38.660868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ncleaned_list = []\nfor title in Raw_list:\n    if isinstance(title, str):\n        # remove square brackets and their contents\n        no_brackets = re.sub(r'\\[.*?\\]', '', title)\n        # replace slashes with spaces\n        no_slashes = re.sub(r'/', ' ', no_brackets)\n        # split the cleaned lyrics into separate lines\n        lines = no_slashes.split('\\n')\n        # remove any remaining empty lines\n        lines = [line for line in lines if line.strip() != '']\n        cleaned_list.extend(lines)\n\ncleaned_list[:3]\n","metadata":{"id":"JKM9Z8dImktF","outputId":"a4033302-e7e7-45f5-c0c3-d0914626112d","execution":{"iopub.status.busy":"2023-11-15T12:42:38.665457Z","iopub.execute_input":"2023-11-15T12:42:38.665815Z","iopub.status.idle":"2023-11-15T12:42:38.692800Z","shell.execute_reply.started":"2023-11-15T12:42:38.665777Z","shell.execute_reply":"2023-11-15T12:42:38.691466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(cleaned_list)","metadata":{"id":"ZGZrbk21myOd","outputId":"77b524e1-8f78-41ea-8519-b1040699b3e3","execution":{"iopub.status.busy":"2023-11-15T12:42:38.694153Z","iopub.execute_input":"2023-11-15T12:42:38.694495Z","iopub.status.idle":"2023-11-15T12:42:38.705437Z","shell.execute_reply.started":"2023-11-15T12:42:38.694465Z","shell.execute_reply":"2023-11-15T12:42:38.704117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corp = cleaned_list[:len(cleaned_list)//4]","metadata":{"id":"BvYBhYbWm2ld","execution":{"iopub.status.busy":"2023-11-15T12:42:38.706767Z","iopub.execute_input":"2023-11-15T12:42:38.707188Z","iopub.status.idle":"2023-11-15T12:42:38.716163Z","shell.execute_reply.started":"2023-11-15T12:42:38.707154Z","shell.execute_reply":"2023-11-15T12:42:38.714968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to lowercase and save as a list\ncorpus = [line.lower() for line in corp]\n\nprint(f\"There are {len(corpus)} lines of text\\n\")\nprint(f\"The first 5 lines look like this:\\n\")\nprint(corpus[:20])","metadata":{"id":"DuyXJLkBm5l1","outputId":"46b75166-95d0-476a-e67d-be504e8ebe79","execution":{"iopub.status.busy":"2023-11-15T12:42:38.717687Z","iopub.execute_input":"2023-11-15T12:42:38.718160Z","iopub.status.idle":"2023-11-15T12:42:38.733169Z","shell.execute_reply.started":"2023-11-15T12:42:38.718120Z","shell.execute_reply":"2023-11-15T12:42:38.731737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\ntotal_words = len(tokenizer.word_index) + 1","metadata":{"id":"R9FU9u88m8w0","execution":{"iopub.status.busy":"2023-11-15T12:42:38.735760Z","iopub.execute_input":"2023-11-15T12:42:38.736219Z","iopub.status.idle":"2023-11-15T12:42:38.774858Z","shell.execute_reply.started":"2023-11-15T12:42:38.736178Z","shell.execute_reply":"2023-11-15T12:42:38.773433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_words","metadata":{"id":"ttYxAH4_m-UE","outputId":"50d17f13-0998-4201-8097-078b5552642c","execution":{"iopub.status.busy":"2023-11-15T12:42:38.778306Z","iopub.execute_input":"2023-11-15T12:42:38.778677Z","iopub.status.idle":"2023-11-15T12:42:38.785592Z","shell.execute_reply.started":"2023-11-15T12:42:38.778646Z","shell.execute_reply":"2023-11-15T12:42:38.784398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus[0]","metadata":{"id":"XFSXCUphm_92","outputId":"9a3b3a8e-583f-4066-a416-ba94b384fc82","execution":{"iopub.status.busy":"2023-11-15T12:42:38.786810Z","iopub.execute_input":"2023-11-15T12:42:38.787176Z","iopub.status.idle":"2023-11-15T12:42:38.798286Z","shell.execute_reply.started":"2023-11-15T12:42:38.787146Z","shell.execute_reply":"2023-11-15T12:42:38.796875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.texts_to_sequences([corpus[0]])[0]","metadata":{"id":"piD7kuadnCel","outputId":"59b5b4a1-f0bb-4121-b4d1-d0f7debfcdd1","execution":{"iopub.status.busy":"2023-11-15T12:42:38.799585Z","iopub.execute_input":"2023-11-15T12:42:38.800093Z","iopub.status.idle":"2023-11-15T12:42:38.810859Z","shell.execute_reply.started":"2023-11-15T12:42:38.800056Z","shell.execute_reply":"2023-11-15T12:42:38.809710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def n_gram_seqs(corpus, tokenizer):\n    \"\"\"\n    Generates a list of n-gram sequences\n\n    Args:\n        corpus (list of string): lines of texts to generate n-grams for\n        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n\n    Returns:\n        input_sequences (list of int): the n-gram sequences for each line in the corpus\n    \"\"\"\n    input_sequences = []\n\n    for line in corpus:\n      token_list = tokenizer.texts_to_sequences([line])[0]\n      for i in range(1, len(token_list)):\n          n_gram_sequence = token_list[:i+1]\n          input_sequences.append(n_gram_sequence)\n\n    return input_sequences","metadata":{"id":"1bJD7CtTnFEc","execution":{"iopub.status.busy":"2023-11-15T12:42:38.811885Z","iopub.execute_input":"2023-11-15T12:42:38.812355Z","iopub.status.idle":"2023-11-15T12:42:38.823039Z","shell.execute_reply.started":"2023-11-15T12:42:38.812316Z","shell.execute_reply":"2023-11-15T12:42:38.821470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the function with one example\nfirst_example_sequence = n_gram_seqs([corpus[0]], tokenizer)\n\nprint(\"n_gram sequences for first example look like this:\\n\")\nfirst_example_sequence","metadata":{"id":"kvkfoTcjnGB0","outputId":"2c8fbe8b-3b1d-4bb9-ca9d-3ac5a6a7a9d4","execution":{"iopub.status.busy":"2023-11-15T12:42:38.824251Z","iopub.execute_input":"2023-11-15T12:42:38.825184Z","iopub.status.idle":"2023-11-15T12:42:38.837708Z","shell.execute_reply.started":"2023-11-15T12:42:38.825087Z","shell.execute_reply":"2023-11-15T12:42:38.836603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the function with a bigger corpus\nnext_3_examples_sequence = n_gram_seqs(corpus[1:4], tokenizer)\n\nprint(\"n_gram sequences for next 3 examples look like this:\\n\")\nnext_3_examples_sequence","metadata":{"id":"ZNQjEm5BnJI1","outputId":"3c0c7c15-3264-4f37-87f0-9549ce637266","execution":{"iopub.status.busy":"2023-11-15T12:42:38.839049Z","iopub.execute_input":"2023-11-15T12:42:38.839468Z","iopub.status.idle":"2023-11-15T12:42:38.853858Z","shell.execute_reply.started":"2023-11-15T12:42:38.839437Z","shell.execute_reply":"2023-11-15T12:42:38.852426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the n_gram_seqs transformation to the whole corpus\ninput_sequences = n_gram_seqs(corpus, tokenizer)\n\n# Save max length\nmax_sequence_len = max([len(x) for x in input_sequences])\n\nprint(f\"n_grams of input_sequences have length: {len(input_sequences)}\")\nprint(f\"maximum length of sequences is: {max_sequence_len}\")","metadata":{"id":"-FypDIJFnLn8","outputId":"260e36d5-98d2-4bee-ed9e-5c67e6b64f9c","execution":{"iopub.status.busy":"2023-11-15T12:42:38.855376Z","iopub.execute_input":"2023-11-15T12:42:38.855726Z","iopub.status.idle":"2023-11-15T12:42:38.886137Z","shell.execute_reply.started":"2023-11-15T12:42:38.855697Z","shell.execute_reply":"2023-11-15T12:42:38.885095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad_seqs(input_sequences, maxlen):\n    \"\"\"\n    Pads tokenized sequences to the same length\n\n    Args:\n        input_sequences (list of int): tokenized sequences to pad\n        maxlen (int): maximum length of the token sequences\n\n    Returns:\n        padded_sequences (array of int): tokenized sequences padded to the same length\n    \"\"\"\n    padded_sequences = pad_sequences(input_sequences, maxlen=maxlen, padding='pre')\n\n    return padded_sequences","metadata":{"id":"iTnl2ivOnNqk","execution":{"iopub.status.busy":"2023-11-15T12:42:38.887403Z","iopub.execute_input":"2023-11-15T12:42:38.887713Z","iopub.status.idle":"2023-11-15T12:42:38.895669Z","shell.execute_reply.started":"2023-11-15T12:42:38.887686Z","shell.execute_reply":"2023-11-15T12:42:38.894269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the function with the n_grams_seq of the first example\nfirst_padded_seq = pad_seqs(first_example_sequence, max([len(x) for x in first_example_sequence]))\nfirst_padded_seq","metadata":{"id":"SkT9C9zYnPol","outputId":"e60d7f61-fa0e-4dc9-c44e-a69e56db331f","execution":{"iopub.status.busy":"2023-11-15T12:42:38.897141Z","iopub.execute_input":"2023-11-15T12:42:38.897482Z","iopub.status.idle":"2023-11-15T12:42:38.914583Z","shell.execute_reply.started":"2023-11-15T12:42:38.897453Z","shell.execute_reply":"2023-11-15T12:42:38.913455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the function with the n_grams_seq of the next 3 examples\nnext_3_padded_seq = pad_seqs(next_3_examples_sequence, max([len(s) for s in next_3_examples_sequence]))\nnext_3_padded_seq","metadata":{"id":"aE0vvbPwnScE","outputId":"d2f35ec2-70f0-410f-a8fc-372336a3b3fa","execution":{"iopub.status.busy":"2023-11-15T12:42:38.916647Z","iopub.execute_input":"2023-11-15T12:42:38.917091Z","iopub.status.idle":"2023-11-15T12:42:38.929185Z","shell.execute_reply.started":"2023-11-15T12:42:38.917054Z","shell.execute_reply":"2023-11-15T12:42:38.927525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pad the whole corpus\ninput_sequences = pad_seqs(input_sequences, max_sequence_len)\n\nprint(f\"padded corpus has shape: {input_sequences.shape}\")","metadata":{"id":"XpqA_WX1nUm1","outputId":"99af7c37-2692-4001-8974-2c31e10e84bc","execution":{"iopub.status.busy":"2023-11-15T12:42:38.931208Z","iopub.execute_input":"2023-11-15T12:42:38.931640Z","iopub.status.idle":"2023-11-15T12:42:38.981326Z","shell.execute_reply.started":"2023-11-15T12:42:38.931608Z","shell.execute_reply":"2023-11-15T12:42:38.980154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def features_and_labels(input_sequences, total_words):\n    \"\"\"\n    Generates features and labels from n-grams\n\n    Args:\n        input_sequences (list of int): sequences to split features and labels from\n        total_words (int): vocabulary size\n\n    Returns:\n        features, one_hot_labels (array of int, array of int): arrays of features and one-hot encoded labels\n    \"\"\"\n    input_sequences = np.array(input_sequences)\n    features, labels = input_sequences[:,:-1], input_sequences[:,-1]\n    one_hot_labels = to_categorical(labels, num_classes=total_words)\n\n    return features, one_hot_labels","metadata":{"id":"mpHD7PkYnYZb","execution":{"iopub.status.busy":"2023-11-15T12:42:38.982630Z","iopub.execute_input":"2023-11-15T12:42:38.982971Z","iopub.status.idle":"2023-11-15T12:42:38.989700Z","shell.execute_reply.started":"2023-11-15T12:42:38.982941Z","shell.execute_reply":"2023-11-15T12:42:38.988524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the function with the padded n_grams_seq of the first example\nfirst_features, first_labels = features_and_labels(first_padded_seq, total_words)\n\nprint(f\"labels have shape: {first_labels.shape}\")\nprint(\"\\nfeatures look like this:\\n\")\nfirst_features","metadata":{"id":"TwTHWrKSnba7","outputId":"7805f4f3-5c78-440b-a57b-4fb6f2c08baf","execution":{"iopub.status.busy":"2023-11-15T12:42:38.991537Z","iopub.execute_input":"2023-11-15T12:42:38.991915Z","iopub.status.idle":"2023-11-15T12:42:39.005567Z","shell.execute_reply.started":"2023-11-15T12:42:38.991885Z","shell.execute_reply":"2023-11-15T12:42:39.004104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the whole corpus\nfeatures, labels = features_and_labels(input_sequences, total_words)\n\nprint(f\"features have shape: {features.shape}\")\nprint(f\"labels have shape: {labels.shape}\")","metadata":{"id":"peLlFpfCneGL","outputId":"63fe61eb-8d70-4d51-8b20-fc997cd6f3f3","execution":{"iopub.status.busy":"2023-11-15T12:42:39.007529Z","iopub.execute_input":"2023-11-15T12:42:39.008112Z","iopub.status.idle":"2023-11-15T12:42:39.051768Z","shell.execute_reply.started":"2023-11-15T12:42:39.007979Z","shell.execute_reply":"2023-11-15T12:42:39.050709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(total_words, max_sequence_len):\n    model = Sequential()\n\n    model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n    model.add(Bidirectional(LSTM(150, return_sequences=False)))\n    model.add(Dense(total_words, activation='softmax'))\n\n\n    # Compile the model\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n\n    return model","metadata":{"id":"3b-ilEWHnggb","execution":{"iopub.status.busy":"2023-11-15T12:42:39.053828Z","iopub.execute_input":"2023-11-15T12:42:39.054720Z","iopub.status.idle":"2023-11-15T12:42:39.063578Z","shell.execute_reply.started":"2023-11-15T12:42:39.054676Z","shell.execute_reply":"2023-11-15T12:42:39.062138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the untrained model\nmodel = create_model(total_words, max_sequence_len)\n\n# Train the model\nhistory = model.fit(features, labels, epochs=20, verbose=1)","metadata":{"id":"WJXdjtBhnkD8","outputId":"a6f9c1ce-f607-4526-a0c0-588d2156709e","execution":{"iopub.status.busy":"2023-11-15T12:42:39.070468Z","iopub.execute_input":"2023-11-15T12:42:39.071803Z","iopub.status.idle":"2023-11-15T12:49:06.620703Z","shell.execute_reply.started":"2023-11-15T12:42:39.071758Z","shell.execute_reply":"2023-11-15T12:49:06.618906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take a look at the training curves of the model\n\nacc = history.history['accuracy']\nloss = history.history['loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training accuracy')\nplt.title('Training accuracy')\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.title('Training loss')\nplt.legend()\n\nplt.show()","metadata":{"id":"tJSK8kFWntJk","outputId":"c6f40b01-3933-4932-a6d4-69d5847a75d8","execution":{"iopub.status.busy":"2023-11-15T12:49:06.622625Z","iopub.execute_input":"2023-11-15T12:49:06.623079Z","iopub.status.idle":"2023-11-15T12:49:07.296647Z","shell.execute_reply.started":"2023-11-15T12:49:06.623035Z","shell.execute_reply":"2023-11-15T12:49:07.295388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_text = \"Elon Musk\"\nnext_words = 10\n\nfor _ in range(next_words):\n    # Convert the text into sequences\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    # Pad the sequences\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    # Get the probabilities of predicting a word\n    predicted = model.predict(token_list, verbose=0)\n    # Choose the next word based on the maximum probability\n    predicted = np.argmax(predicted, axis=-1).item()\n    # Get the actual word from the word index\n    output_word = tokenizer.index_word[predicted]\n    # Append to the current text\n    seed_text += \" \" + output_word\n\nprint(seed_text)","metadata":{"id":"qiArEi25nttM","outputId":"f7b75a66-1e56-49f2-f2fa-f28a4314bf33","execution":{"iopub.status.busy":"2023-11-15T12:49:07.298098Z","iopub.execute_input":"2023-11-15T12:49:07.298912Z","iopub.status.idle":"2023-11-15T12:49:08.942359Z","shell.execute_reply.started":"2023-11-15T12:49:07.298844Z","shell.execute_reply":"2023-11-15T12:49:08.940812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"S9BMob8ILNSW"},"execution_count":null,"outputs":[]}]}